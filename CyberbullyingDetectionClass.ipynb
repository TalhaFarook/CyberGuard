{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe52612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Offensive Words:  ['retard', 'weirdo', 'beaner'] \n",
      "Severity Level:  3.6666666666666665 \n",
      "Type:  ['ethnicityandrace']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#import requests as rq\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#!pip install wordninja\n",
    "import wordninja\n",
    "\n",
    "#!pip install spacy\n",
    "#import spacy\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#!pip install contractions\n",
    "import contractions\n",
    "\n",
    "#!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score   \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "import pickle\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",\n",
    "    password = \"my123SQL$\",\n",
    "    database = \"modeldata\"\n",
    ")\n",
    "\n",
    "command = mydb.cursor()\n",
    "\n",
    "class CyberbullyingDetection():\n",
    "    def string(self, data):\n",
    "        string = ' '\n",
    "        return (string.join(data))\n",
    "\n",
    "    def detectBullying(self, text):\n",
    "        tweet = pd.DataFrame(text, columns=['tweets_text'])\n",
    "        tweet = tweet.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "        \n",
    "        tweet['tweets_text'] = tweet['tweets_text'].str.lower()\n",
    "        \n",
    "        temp = ''\n",
    "        for index, row in enumerate(tweet['tweets_text']):\n",
    "            temp = re.sub(r'(\\brt)|(http\\S+)|(\\d+)|(&(gt;)+)|(&(lt;)+)|(&(amp;)+)|([^\\w\\s])', '', str(row))\n",
    "            temp = re.sub('(\\'| )|(\\\"| )|(_)', ' ', temp)\n",
    "            tweet['tweets_text'][index] = temp\n",
    "        \n",
    "        for index, row in enumerate(tweet['tweets_text']):\n",
    "            temp = []\n",
    "            for word in row.split():\n",
    "                temp.append(contractions.fix(word))\n",
    "            tweet['tweets_text'][index] = self.string(temp)         \n",
    "            \n",
    "        command.execute(\"SELECT * FROM slang\")\n",
    "        slangWords = pd.DataFrame(command, columns=['slang', 'word'])\n",
    "        slangWords = slangWords.replace(to_replace= '\\\\r', value= '', regex=True)    \n",
    "        \n",
    "        for num, row in enumerate(tweet['tweets_text']):\n",
    "            temp = []\n",
    "            for word in row.split():\n",
    "                found = 0\n",
    "                if (len(word)<6 and len(word)>2): \n",
    "                    for index, slang in enumerate(slangWords['slang']):\n",
    "                        if (slang == word):\n",
    "                            temp.append(slangWords['word'][index])\n",
    "                            found = 1\n",
    "                if (found != 1):\n",
    "                    temp.append(word)\n",
    "            tweet['tweets_text'][num] = self.string(temp)\n",
    "            \n",
    "        for index, row in enumerate(tweet['tweets_text']):\n",
    "            temp = []\n",
    "            for word in row.split():\n",
    "                if (len(word)>8):\n",
    "                    unmunched = wordninja.split(word)\n",
    "                    temp.append(self.string(unmunched))\n",
    "                else:\n",
    "                    temp.append(word)\n",
    "            tweet['tweets_text'][index] = self.string(temp)\n",
    "            \n",
    "        tokens = []\n",
    "        for row in tweet['tweets_text']:\n",
    "            tokens.append(word_tokenize(row))\n",
    "        \n",
    "        tweet['tokens'] = tokens   \n",
    "        \n",
    "        command.execute(\"SELECT * FROM offensivewithseverity\")\n",
    "        offenseWords = pd.DataFrame(command, columns=['word', 'severity'])\n",
    "        offenseWords = offenseWords.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "        \n",
    "        command.execute(\"SELECT * FROM negation\")\n",
    "        negationWords = pd.DataFrame(command, columns=['word'])\n",
    "        negationWords = negationWords.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "\n",
    "        totalWords, offensiveWords, severityWords = [], [], []\n",
    "\n",
    "        for row in tweet['tokens']:\n",
    "            words, temp1, temp2 = 0, [], []\n",
    "            for index1, token in enumerate(row):\n",
    "                words += 1\n",
    "                for index2, offensive in enumerate(offenseWords['word']):\n",
    "                    if (token == offensive):\n",
    "                        negation = 0\n",
    "                        for negation in negationWords['word']: #Checking for negation words at most 2 words before the negative word \n",
    "                            if (index1<1):\n",
    "                                break\n",
    "                            if (row[index1-1] == negation or row[index1-2] == negation):\n",
    "                                negation = 1\n",
    "                                break\n",
    "                        if (negation != 1):\n",
    "                            temp1.append(token)\n",
    "                            temp2.append(offenseWords['severity'][index2])\n",
    "            totalWords.append(words)\n",
    "            offensiveWords.append(temp1)\n",
    "            severityWords.append(temp2)\n",
    "\n",
    "        tweet['total words'] = totalWords\n",
    "        tweet['offensive words'] = offensiveWords\n",
    "        tweet['severity words'] = severityWords\n",
    "        \n",
    "        density = []\n",
    "        for total, offensive in zip(tweet['total words'], tweet['offensive words']):\n",
    "            density.append(len(offensive) / total)\n",
    "        tweet['density'] = density\n",
    "        \n",
    "        compound = []\n",
    "        for row in tweet['tweets_text']:\n",
    "            polarity = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "            compound.append(polarity[\"compound\"])\n",
    "        tweet['sentiment analysis'] = compound\n",
    "        \n",
    "        severity, weights = [], [1, 2, 3, 4, 5]\n",
    "        for severe in tweet['severity words']:\n",
    "            count, product = [0] * 5, []\n",
    "            for num in severe:\n",
    "                if (num == '1'):\n",
    "                    count[0] += 1\n",
    "                elif (num == '2'):\n",
    "                    count[1] += 1 \n",
    "                elif (num == '3'):\n",
    "                    count[2] += 1\n",
    "                elif (num == '4'):\n",
    "                    count[3] += 1 \n",
    "                elif (num == '5'):\n",
    "                    count[4] += 1       \n",
    "            for num1, num2 in zip(count, weights):\n",
    "                product.append(num1 * num2)\n",
    "\n",
    "            totalProduct = sum(product)\n",
    "            totalCount = sum(count)\n",
    "\n",
    "            if (totalCount == 0):\n",
    "                severity.append(0)\n",
    "            else:\n",
    "                severity.append(totalProduct / totalCount)\n",
    "\n",
    "        tweet['severity'] = severity\n",
    "        \n",
    "        tweetDataM1 = tweet[['density', 'severity', 'sentiment analysis']].copy()\n",
    "        tweetDataM1.head()\n",
    "        \n",
    "        model = pickle.load(open(\"cyberbullyingdetection.sav\", 'rb'))\n",
    "        \n",
    "        cyberTarget = model.predict(tweetDataM1)\n",
    "        if self.string(cyberTarget) == 'cyberbullying':\n",
    "            tweet['cyberbullying'] = 'True'\n",
    "        else:\n",
    "            tweet['cyberbullying'] = 'False'\n",
    "            \n",
    "        if (tweet['cyberbullying'].values == 'True'):   \n",
    "            command.execute(\"SELECT * FROM ethnicityAndRaceGlossary\")\n",
    "            ethnicityAndRaceGlossary = pd.DataFrame(command, columns=['word'])\n",
    "            ethnicityAndRaceGlossary = ethnicityAndRaceGlossary.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "            ethnicityAndRaceGlossary.head()\n",
    "\n",
    "            command.execute(\"SELECT * FROM ageGlossary\")\n",
    "            ageDataGlossary = pd.DataFrame(command, columns=['word'])\n",
    "            ageDataGlossary = ageDataGlossary.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "            ageDataGlossary.head()\n",
    "\n",
    "            command.execute(\"SELECT * FROM genderGlossary\")\n",
    "            genderDataGlossary = pd.DataFrame(command, columns=['word'])\n",
    "            genderDataGlossary = genderDataGlossary.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "            genderDataGlossary.head()\n",
    "\n",
    "            command.execute(\"SELECT * FROM religionGlossary\")\n",
    "            religiousDataGlossary = pd.DataFrame(command, columns=['word'])\n",
    "            religiousDataGlossary = religiousDataGlossary.replace(to_replace= '\\\\r', value= '', regex=True)\n",
    "            religiousDataGlossary.head()\n",
    "\n",
    "            isEthnicityAndRace = []\n",
    "            for row in tweet['tokens']:\n",
    "                temp = 0\n",
    "                for token in row:\n",
    "                    for glossary in ethnicityAndRaceGlossary['word']:\n",
    "                        if (token == glossary):\n",
    "                            temp += 1\n",
    "                            break\n",
    "                isEthnicityAndRace.append(temp)\n",
    "\n",
    "            tweet['ethnicity and race'] = isEthnicityAndRace\n",
    "\n",
    "            isAge = []\n",
    "            for row in tweet['tokens']:\n",
    "                temp = 0\n",
    "                for token in row:\n",
    "                    for glossary in ageDataGlossary['word']:\n",
    "                        if (token == glossary):\n",
    "                            temp += 1\n",
    "                            break\n",
    "                isAge.append(temp)\n",
    "\n",
    "            tweet['age'] = isAge\n",
    "\n",
    "            isGender = []\n",
    "            for row in tweet['tokens']:\n",
    "                temp = 0\n",
    "                for token in row:\n",
    "                    for glossary in genderDataGlossary['word']:\n",
    "                        if (token == glossary):\n",
    "                            temp += 1\n",
    "                            break\n",
    "                isGender.append(temp)\n",
    "\n",
    "            tweet['gender'] = isGender  \n",
    "\n",
    "            isReligious = []\n",
    "            for row in tweet['tokens']:\n",
    "                temp = 0\n",
    "                for token in row:\n",
    "                    for glossary in religiousDataGlossary['word']:\n",
    "                        if (token == glossary):\n",
    "                            temp += 1\n",
    "                            break\n",
    "                isReligious.append(temp)\n",
    "\n",
    "            tweet['religion'] = isReligious\n",
    "\n",
    "            tweetDataM2 = tweet[['age', 'gender', 'religion', 'ethnicity and race']].copy()\n",
    "\n",
    "            model = pickle.load(open(\"cyberbullyingtype.sav\", 'rb'))\n",
    "\n",
    "            classifyTarget = model.predict(tweetDataM2)\n",
    "\n",
    "            print('\\nOffensive Words: ', tweet['offensive words'][0], '\\nSeverity Level: ', tweet['severity'][0], '\\nType: ', classifyTarget)\n",
    "    \n",
    "        else:\n",
    "            print(\"No offensive words!\")\n",
    "\n",
    "text = [\"You're A retardHISPANIC, all you do is drink tequilla and mow lawns you weirdo beaner!\"];\n",
    "scan = CyberbullyingDetection()\n",
    "scan.detectBullying(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
